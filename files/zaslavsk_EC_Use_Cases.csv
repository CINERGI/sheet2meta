"Internal ID","Use Case filename#long","#href","Summarized by","Summary Date#date","Title","Domain Keywords#multi","EC Science Themes#multi","Interdisciplinarity (Low, Med, High)","Overall Goal#long","Main CI Challenges#long","Data Discovery Challenges#long","Data Types and Formats#long","Software Used#long","Standards Used","Additional Information#long","Internal notes#long","","","","","","","","","","","","","","","","","","","","","","","","#img","#name"
"31","Lake Tahoe as an example of a large inland water body study--Geoffrey Schladow","","KS","8/22/2016","Lake Tahoe as an Example of a Large Inland Water Body Study","Limnology","Long-term trends|State parameters|Sources of variability","?Med","To combine real-time and actively sampled data to understand how physical processes in Lake Tahoe, over both short term and long terms, are impacted by ecological change in the area.","- have 3-4 home-grown database systems for their multiple data types that take effort to develop, aren't compatible with each other, and are problematic when someone retires or budgets fluctuate. Don't have the money for Oracle, don't have CS expertise to build better, and their scientists are not well-trained in this. \n- There is a need for real-time quality controlled data, But the QC processes are time consuming, so this can't be met. Would like automated, artificial intelligence approach to real-time validations. ""this may be the biggest ongoing dilemma.""\n- Having one data access architecture serve different user types with different needs/expectations \n- can't calibrate most instruments remotely\n- some data needs to be collected in-person","- serving data to diverse users (public wants downsampled, science wants high temporal resolution, etc) using home-grown systems developed with little CS expertise.","- Data Types: ecological and water quality data station, processed image data, simulation data, sequence data. Imported LIDAR data.\n- Data Sources: in situ sensors (field data) - streaming and manual readings. Model and analytic outputs. Molecular sequencing\n- Data Repositories: ""NOAA data sources""\n- Data Formats: CSV (put into SQL database):  essentially GPS location, timestamp, and a set of measured parameters.\n- DAta Volume: ""Huge"": 10MB per day for water quality. 20TB/day for LSST (but not clear if they are archiving this or using it)\n","Software: We utilize geospatial software, MATLAB, and a variety of statistical packages. We use various statistical and database programs where possible as well. \n- SQL for database","- ""there are not standards to the data types we collect or store.""","","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Lake Tahoe as an Example of a Large Inland Water Body Study"
"1","Kathleen Ritterbush Use Case summary","","KS","8/15/2016","Information tracking for a large collaborative deep-time project","Paleontology|Geochemistry|Ecological modeling","Sources of variability|Long-term trends|Implications for others","High","To create a community data portal that integrates paleontological and geochemical data about samples (rocks, photos, microscope slides) for use by those sciences plus ecological modelers. Could support important hypothesis testing.","- no system holds all the information about a sample and what has happened to it, plus the underlying stratigraphic framework. Very time consuming to try to capture information manually, data are all split up in ""weird"" repositories. \n- need to accommodate age models with uncertainty/confidence level in the dating of samples\n- GIS works for the spatial component, but not the temporal component","- wants to be able to find all the information about a sample, whether paleontological or geochemical.","Data Types:  information about samples (field and lab), including microscope slides and images of slides; geochemical data from samples, age models, stratigraphic data\nFormats: GIS, Excel, Pivot Pilot (not clear exact formats)\nRepositories: multiple, not named","Excel\nPivot Pilot\nAdobe Illustrator\nPossibly GIS","none mentioned","Provider would very much like the desired system to actually be developed.","use case needs to be reorganized into template",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Information tracking for a large collaborative deep-time project"
"3","Seismology_CHORDS_science_use_case_Vernon","","KS","8/15/2016","Streaming real-time data for seismic early warning and monitoring and environmental monitoring","Atmospheric processes|Seismology|Natural hazards","Sources of variability|Hazards","High","Develop a real-time streaming network for seismology and atmospheric sensors and cameras (data and processed products), for use by researchers, policy makers, first responders.. Specifically, for Seismology and other areas: \n1) Make it easier to bring in data in real-time. This enhances usefulness by getting the scientific analysis out of it faster and earlier, by making the best quality measurements. \n2) Generating more, better data products that more accurately reflect the actual weather phenomenon.\n3) Capture enough data to get big picture of environment with high-quality timing (correlated timing between sensors)\n","- Funding is insufficient to cover the number of sensors (don't have meteorological sensorss at all monitoring sites) and the sophistication of algorithms he would like. \n- Need to figure out a way to build a sustainable network that�s well engineered and migrated into a sustainable infrastructure.A long term maintenance model is needed.\n","Copied from Use Case\n(These are about data collection but could also apply to saved data)\n- Geometric mapping of different data types and all of the different inputs coming at once is really hard to do\n- Figuring out time scales to take measurements at and then matching them up - for each new variable in each location, timeframe\n- Weather effects can knock out ability to collect data, causing gaps in data\n- Synchronizing data","- Data Sources: variety of in situ sensors giving images, meteorological, seismic, motion accelerometer, and state of health data. \n- Data Formats: miniC (a seismology format). There are MATLAB interfaces for this. Data formats are described on IRIS/DMC web site.\n- Repositories: IRIS DMC\n- Volume: 5TB/yr US, 10TB waveform for whole project.\n- Velocity: real-time (2 sec latency)\n- Variability: lots\n- Veracity: high","Most used: Antelope, MATLAB. \nOthers: Fledermaus (3d 4d vis), ObsPy/Pyton","none","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Streaming real-time data for seismic early warning and monitoring and environmental monitoring"
"4","Watershed_model_Chapela_Lara_Full_V2","","KS","8/15/2016","A dynamical watershed model for concentration-age-discharge in a highly weathered tropical site (Luquillo CZO).","Computational hydrology (geophysics)","Predictions|Sources of variability","High","Improve knowledge of the relationship between chemical weathering, nutrient cycling and hydrology by developing model repositories and online access to data to support the hydrological parameters","1. Not all data are discoverable and accessible - LTER Catalog helps, but is not complete, so individual scientists need to be asked for their data. \n2. The data require QC and manipulation that is time consuming. Some of the QC could be automated if community scripts were available. Variations in parameter names and units also made time consuming, and metadata was not always complete\n3. There were gaps in the data, making it hard to find sites and timespans with complete data.","I also spent a lot of time finding the right parameters, as different terms are used exchangeably or change through historical records, or units that are common ground for hydrologists/climatologist, but were unknown to me. While usually I could solve that by looking at the explanation in the LCZO metadata, the LTER one is less clear, and sometimes I had to contact the data manager directly or look up several books. This could be easily solved having a hydro-Glossary or similar, as well as common units and abbreviations.\nMost of the problems I encountered can be grouped as:\na)        Missing values\nb)        Values at sampling intervals longer than required by the model\nc)        Values at sampling intervals shorter than required by the model\nd)        Different units\n\nPart of his search is that she wants to find sites and timespans with complete data. Not just do data exist in that place/time, but is it complete. ","- Data Sources: LTER Data Catalog, LCZO Data Catalog, published papers, *and* privately held, unpublished data. \n- Data Format: Mostly .csv files, few .txt and excel. The only data heterogeneity problem was with dates and with units. CDF (Computable Document Framework) format or documenting models that provides the basic theory, equations and numerical solution along with visualization of results\n- Repositories: Luquillo LTER data catalog, LCZO repository\n- Variety (multiple datasets, mashup): Data heterogeneity was an important challenge, both managing the variety and figuring out which are best suited to the model\n- Variability: Missing data (gaps) was a large challenge\n- Veracity/Data Quality (accuracy, precision): Reliability of some of the data was an issue, and could not have been spotted without hydrology expertise; asking the LCZO data manager and PI was needed to finally discard some data. \n- Data Types: All were time series, except for well logs and map of the site","- GitHub: a tool for sharing software and tracking versions.\n- GeoSoft model library\n- The model Maria based her customized model on, residing in GitHub\n- Maria�s customized model, a changed version of the above.\n- Matlab and Mathematica: the environment in which Maria edited the model.\n- Skype: for communication","EML (Ecological Metadata Language)\nISO 19115 metadata\nDOIs","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","A dynamical watershed model for concentration-age-discharge in a highly weathered tropical site (Luquillo CZO)."
"11","https://drive.google.com/drive/folders/0B-WQtNtnyJLTSXQ3MU8yd1FsdFU","","LK","8/24/2016","EarthCollab data center use cases_Mayernik","Polar programs|Earth Science|CISE|AGS|OCE","unclear","High","1) Developing of a novel semantic web cyberinfrastructure using Vivo software; and \n2) Developing semantic web models specific to the needs of earth sciences by using the data center use cases of UNAVCO�s geodesy research and a Bering Sea project to provide the technical details and domain science content for the models. \n","- Challenge: Getting the links established between components in the semantic web, e.g. between publications and datasets, is time consuming manual process now but could be done with software. This is more of a human problem than a technology problem, as standards are just starting to be developed to help guide people�s documentation of information. There are many people working on trying to solve this problem. Some solutions underway in the larger community: 1) Assigning DOI�s to projects and project components (e.g., datasets), and 2) Create standards among publishers so that these connections and tracking capabilities happen at the point of publication. Efforts made in their project: They are using a combination of search engines for data mining and manual searching. Manual searching is working fine for their small project (on the order of thousands of data entries), but it is not a scalable solution. As their project grows, this is likely to become a problem.\n- Challenge: Data variety. Without available standards, each data center/institute uses different formats. This data variety is a challenge because all must be converted to RDF, so that they can be used by Vivo. For example, they must map time ranges into an RDF structure, and this is time-consuming. It would be nice if this process was more efficient. This is a challenge that all semantic web projects face. On the other hand, the diversity in the data formats is also beneficial. If you try to standardize everything, it takes away the value that diversity also has to offer (e.g., more opportunities for innovation). Efforts: Currently, they do all of this translation work manually. They are writing translation tools themselves with the help of software engineers. ","","�","Vivo","RDF\nOntology - VIVO-ISF, GCIS, DCAT","This is an EC funded project in support of data discovery, not a scientist seeking to solve a science problem, per se.","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","EarthCollab data center use cases_Mayernik"
"15","Stromatolite Distribution In Space And Time_Peters.docx","","KS","8/19/2016","Stromatolite Distribution In Space And Time","Genetics|Paleontology|Mineralogy","None","High","To use GeoDeepDive to compile data and data synthesis on the frequency (e.g., peaks and declines) of stromatolite occurrence in space and time as a source of information about the evolution of earth�s surface environmental conditions.\n","- before GeoDeepDive: data on stromatolite occurrence data were distributed in diverse scientific literature, and difficult and time consuming to access & extract.\n- Continuing challenges for GeoDeepDive: CPU is limiting, require coding skills to use that many students do not have, data quality problems, not able to find all data in a paper, etc.\n- how to credit data aggregations from GeoDeepDive is not fully resolved - can list the papers, but not all contribute equally. Need community best practices around this.","- finding data on stromatolite occurences in published scientific literature\n- as literature is added to GeoDeepDive, may want to repeat the search, but not clear if there is an efficient way to retrieve just the new hits.","- Data Formats: PDFs\n- Data Types: stromatolite distributions in space and time, from published literature\n- Data Repositories: published literature collections\n- Data Volume: could be up to 100s of GBs. Problematic volume for personal computers. \n- Data velocity, variety, and variability not listed as challenges","GeoDeepDive (use case was provided by GeoDeepDive team)","Open source, standard formats, annotation provided by software toolkit. Eg. Stanford NLP","EC GeoDeepDive (use case was provided by GeoDeepDive team)","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Stromatolite Distribution In Space And Time"
"17","Mapping River Migration Dynamics From Landsat Imagery_Schwenk","","KS","8/19/2016","Mapping River Migration Dynamics From Landsat Imagery","Geomorphology|Global change|Geographic location","Sources of variability|Hazards|Predictions","High","To understand the migration of rivers through time using Landsat satellite imagery of rivers. Big questions ultimately are: How do humans impact river dynamics? (And how are humans impacted by migrating rivers?) How might climate changes and sediment dynamics affect river migration? ","- processing the images to mark what is water and what is land is manual and very time consuming. He thinks he could develop an algorithm to do it. \n- No appropriate repo exists: he would like to share all of his data products, intermediate products, and tools, but doesn't know where/how. He had to develop all his code, and after he shared it, many people used it, and he was also told someone else had already developed that code (lack of sharing leads to redundant effort)\n- currently requires a supercomputer, but he would like to make data and processes desktop accessible - he thinks this is tractable.\n- having standards and defined methodologies would have helped him\n- data products are large enough that sharing them within his team, and externally, is hard. \n- no good visualization approach for showing the quantified data compactly. ","He is able to access the data he wants from  Google Earth Engine API and/or Earth Explorer (USGS product). This was not noted as a challenge. \n","- Data Formats: Images are downloaded as a *.tiff, he stores data as *.mat and *.m (matlab scripts). Intermediate data products that are raster images that may be in any format (e.g. *.tiff, *.jpeg, *.png, etc.).  \n- Data Repositories: Google Earth Engine API, USGS Earth Explorere (Landsat archive)\n- Data Types: Landsat satellite imagery (input), data on river position and classification (width, cruvature, migration, etc.), maps of river\n- Data Volume: Downloading files from Landsat are about 250 GB, and he processes these down to make them smaller. Processed final map image and intermediate maps for the Ucayali River map are about 2 GB. He is unsure how much data volume would be required to make maps for all of South America. \n - Data Velocity (e.g., real time): Ranges from a few MBs to a few GBs a day. He averages about 0.5 GB a day. Not listed as challenge\n- Data Variety: not a challenge\n- Data Veracity: relies on USGS processing of Landsat. ","Python and Matlab scripts that he wrote. Google Earth Engine","Lack of standards is a challenge","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Mapping River Migration Dynamics From Landsat Imagery"
"22","https://docs.google.com/document/d/1E4Ql_leJ-GRo3Z6Yoo2Ywg2xQdi8uw4G1Sw5HnmY9ns/edit","","LK","8/26/2016","NARCCAP Data for City Management","Global change|Policy sciences","Hazards|Predictions","High","To supply a city's managers (TX?) and others with future scenario temperatures based on the NARCCAP data, so that their team can make city infrastructure decisions.\n","-�Large data transfers not easily supported, so end up sending hard disk drives to supply data\n-�Need tools for extracting the relevant data (or subsampling) based on required info, such as location\n- Need format converters \n- Downloads are�challenging - different internet browsers, security constraints\n-�Analysts choose their own S/W. Different S/W support different data formats\n- Because they assumed the data would never change, it was not versioned and there were no plans to update it\n","- extracting the relevant data from the NA dataset\n- Analysts choose their own S/W. Different S/W support different data formats\n- different OSs have different and less or more tools to work with projected coordinate systems (it doesn't use lat/long\n- Data is in 5-year chunks\n- 30 day/month calendar vs. 365 day/year cause synching issues\n- Units differences, conversion needs\n","netCDF","User dependent, in this example case: ArcGIS","CF metadata\nspecifications on NARCCARP site","Nice process description in use case","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","NARCCAP Data for City Management"
"25","Access to Wisconsin Geologic samples collection_ Stanley.docx","","KS","8/20/2016","Access to Wisconsin Geologic Samples Collection","Informatics|Geochemistry|Geochronology","Long-term trends|Sources of vulnerability|Hazards|Predictions","High","To have a portal to the data and samples in the Wisconsin Geological and Natural History Survey (WGNHS) repository to make them more discoverable.","- need to standardize the collection. Some samples have IGSNs, but not all. Legacy samples don't always have the metadata necessary for an IGSN. would like a more efficient way to get IGSNs\n- no clear place in the workflow where IDs and metadata are captured. Time consuming and effort is limited particularly for legacy sample work\n- need to build a portal ","- They need to build a portal to make their sample information discoverable; but she says this is not hard- there are lots of good exemples out there - but is workd and resources are limited. \n- it would be helpful if there were an easy way for their database to communicate directly with SESAR (the ISGN registry)\n- need to develop metadata templates for uniform data collection. ","- Data Types: Data and metadata about geological samples: geospatial, geochemical, descriptions of data, images, scans, geophysical data, and digital elevation models. Data of the different samples are tied together by depth (i.e., in the earth).  \n- Data Sources: field, chemical analyses (XRF devices), digital elevation model (DEM) output\n- Data repositories: SESAR IGSNs repository. Project hosts their own data on WGNHS servers. \n- Data Formats: Excel .xls, Arcmap shape files, layer files (e.g. lidar dataset), jpeg, nef, .kmz\n- Data Volume: 10.3 TB  \n- Data Velocity, Variability: not stated as challenges\n- Data Veracity/Quality: needs to be addressed. Interviewee is working on a workflow for quality. \n- Data Variety: many different kinds of data need to be brought together. "," ArcMap. ArcMap, Petrel (can view samples through Access, but it�s in Microsoft SQL server), Adobe Photoshop and Illustrator, Microsoft Office, LabelMatrix\nThey are currently deciding on which descriptor software to use. \n","- USGS metadata repository � National Digital Catalog.\n- NCGMP09 � National Geologic Map Database \nEditorial note: these are listed in the standards section, but it is not clear if they are standards, or data repositories/resources.","EC iSamples project is helping with workflows","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Access to Wisconsin Geologic Samples Collection"
"26","https://docs.google.com/document/d/1PwEfIwGt68FuaE1W6DYOEX5JFFYJ1Cuy0osJvo15d9k/edit","","LK ","8/26/2016","Science data system infrastructure","Informatics","None","High","Provide the infrastructure, capabilities, and support for science discovery. Enhance science discovery by providing a data system that can support the science data life cycle. Allow for improved decision-making, and public education and communication. \n","- Project Management:  budgeting and scheduling, finding the right talent, keeping up with emerging technologies, staying technical as much as possible. There is more funding than people to do the work.\n- Identifying end user requirements: scientists don't always know what they want.\n- Getting community buy-in\n- Many different data formats and metadata. People use different terms to describe relationships between data. This makes data systems architecture very complicated. ","","Data Formats: Many - depends on the client. This is a challenge. \nplus Geotiff for visualization\n","Lots, including for cloud computing","OGC\nWeb services","Not a geoscientist use case - is from a group that develops CI to support scientist","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Science data system infrastructure"
"27","Climate Feedback of Mesoscale Cloud-Field Organization Using an Event-Based Approach_Kuo_Nair.docx","","KS","8/20/2016","Climate Feedback of Mesoscale Cloud-Field Organization Using an Event-Based Approach","Informatics","Hazards|Predictions","High","The scientific objectives of the use case are: \n- To examine the evolution (i.e. lifecycle) of cloud-field organization of individual mesoscale arcs, trade wind cumuli, and tropical cloud/storm clusters.\n- To quantify and characterize their evolutions,\n- To accumulate long-term (i.e. decadal) statistics of the evolutions,\n- To identify changes in their long-term characteristics, \n- To ascertain the presence/absence of correlation between their changes, and \n- To discover the feedback mechanism(s) of their changes, if there is correlation.\n","Most earth scientists are research oriented and phenomenon focused. For most events, there are few long-term records that exists. An exception is hurricanes or El Nino events.\n- Currently, scientists move data from their local computer or storage resources before they start their analysis. This process is becoming too unsustainable because the data volume is too large. Also, there are often multiple copies on different computers (e.g., redundancy). [reducing the need to download data was highlighted as a major goal]\n- There are lots of different kinds of data being generated. E.g., Radiometers  - different wavelengths; LiDAR radar - sends out signal; Polarimeters; Synthetic aperture radar, etc. How can a scientist deal with so much variety of data?\n- Model simulations are now conducted at higher resolution, which requires more velocity. This has implications on applications. For example, if there is a disaster, data must be analyzed more quickly for decision-making. \n- How can we be sure that we are collecting and analyzing data accurately? Reproducibility is also needed. ","Investigators queries Para-DIAME to find all of the datasets associated with cloud formation and potential drivers (e.g, land use, and atmospheric characteristics), as well as in-situ and remote sensing observation, in the region of interest.","- Data Types Sources: Radiometers  - different wavelengths; LiDAR radar - sends out signal; Polarimeters; Synthetic aperture radar, etc. Long-term data on cloud formation and evolution; processes, e.g. aerosol and land use, that could impact cloud formation and evolution.\n- Data Repositories: Para-DIAME (is a data and compute environment in the cloud)\n- Data Velocity: high resoluton models sometimes need to be analyzed quickly in cases of disasters\n- Data Veracity/Quality: is a challenge\n- Data Variety: is a challenge due to the many data types\n","- Para-DIAME (is a proposed data and compute environment in the cloud). SciDB: A parallel distributed database management system based on array data model.\n- Spark (on top some supported storage backend)\n- Hadoop (MapReduce) on Hadoop File System, HDFS\n","","Not entirely clear to what degree the Para-DAIME vision presented exists vs. is proposed. ","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Climate Feedback of Mesoscale Cloud-Field Organization Using an Event-Based Approach"
"29","Merging and using diverse datasets of seafloor observations_Rubin.docx","","KS","8/22/2016","Merging and using diverse datasets of seafloor observations","Volcanology|Oceanography|Geochemistry","Sources of Variability|Hazards|Predictions","High","To take observations from oceanic submersible vehicles and construct a geospatial time-series evolution of the sea floor. The originator�s goal within this larger multidisciplinary project is to understand how volcanic landforms form over time, and how volcanic ecosystems work. ","- Determining the precise location of submersible at a point in time for images/samples is difficult, error prone, has uncertainties that are not clear, and is often manual. But is critical for relating multidisciplinary datasets, or sharing data with other disciplines, and for building time series. A tool to help assign coordinates to data/video would be helpful (and improve based on expert opinion through time), as would computational experts that can help with sharing workflows and automating some components. \n- changing technology, e.g. needing to digitize VHS tapes. \n- searching 100s of hours of video and thousands of stills that have virtually no annotation. Finding video/images of interest is time consuming and manual. Would benefit from a more standard way to tag pictures and video for later review, as well as best practices to support consistency.  (scientists take notes on observations from video in real time, but it is not clear what format these notes are in) \n- ArcGIS is the primary tool for his work, but uses a proprietary format that is hard to share or merge with colleagues that don't use ESRI. ","- searching 100s of hours of video and thousands of stills that have virtually no annotation. Finding video/images of interest is time consuming and manual. Would benefit from a more standard way to tag pictures and video for later review, as well as best practices to support consistency.  (scientists take notes on obsevations from video in real time, but it is not clear what format these notes are in) ","- Data Type: video and still imagery of the seafloor \n- Data Source: collected from various submseribles, AUVs, ROVs, towed cameras, etc. There are also sensors on the platforms. \n- Data Formats: ArcGIS internal, NetCDF, CSV\n- Data Volume: 100TB per expedition, with 1-4 expeditions per year. 95% is video/imagery","ArcGIS, a bit of QGIS, Fledermaus (Google Earth and ArcScene are alternatives), VLC video imaging and editing, Excel. ","- WOCE quality assessment convention and flags.\n- Observation notes are taken using a standardized language that is codified in the literature. Ditto for classification schemes. \n- GPS navigation and reporting datums","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Merging and using diverse datasets of seafloor observations"
"30","https://drive.google.com/open?id=0BzqmuwuUSSbtSXhWUWd3Vjljcmc","","LK","8/26/2016","Inland water communities and water chemistry (DDH)","Geochemistry|Hydrology","Sources of variability","High","To understand the capacity of rivers to transfer chemical signals into the ocean. This is about the interaction about inland waterways and oceans, generally, mixing of chemicals, pollutants tranfer, etc.","Data not reliably compiled, except that PANGAEA has taken it on (EMEA).� Plus USGS info is not (easily) accessible to non-USGS affiliates (Scribe: Why not?)","See CI challenges ","UTF-8 Unicode","","UTF-8 Unicode","","NEED TO RECONCILE DUPLICATE VERSIONS, and massage format.This is a DDH use case.",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Inland water communities and water chemistry (DDH)"
"34","https://drive.google.com/open?id=1kbTPco3R9ymTWGd2bx_7y2ZbCgq4NWSUlIH_pYFAQ9g","","LK","9/13/2016","Repeatable and Reproducible Geoscience Research"," Reproducible research","","High","reuse of data models and data preparation workflows, developed across several hydrology laboratories, and which must be reproduced for easy use by other members of the community.","Many hydrologica models have been developed, but the models and workflows using them are not easily reused. Geounit methodology is a solution needs adoption.","If tools don't support the mechanisms for annotation, it's difficult to get buy-in to use the annotation","Data Types and Repositories: \n -ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n -NCDC Global Historical Climatology Network (GHCND) database\n -Meteorological data from NCDC �GHCND\n -Topography (shape file/DEM) from USGSHyDRO1K   http://www.usgs.gov/  \n\nData Formats: Mostly text, time-series data. Example: netCDF, .csv, ,grib, \n\nData Volume (size): Input Data: 1628.14 MB.  Output Data: 1002.13","Graphviz (.gv)","netCDF\nBagIT","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Repeatable and Reproducible Geoscience Research"
"35","Field Mapping and Collecting Structural Field Data - Steven Whitmeyer","","KS","8/22/2016","Field Mapping and Collecting Structural Field Data","Tectonophysics|Education|Structural geology","Long-term trends|Sources of variability","High","Develop a 4D framework for holding diverse geospatial data from field areas (stratigraphic, paleontologic, geochronological, etc) and an app for downloading a section of data of interest for maps to take into the field and upload new data. The ultimate goal is to build 4D tectonic models of geologic evolution","- a diversity of data relevant to 4D tectonic models exists, but is not easily integrated with each other spatially (into maps), or connected to models. The goal is maps with a historical (time) context. Collect point, line, and polygon data in the field into a data structure that would have fields for associated information. This information will include lithology, structural orientations, stratigraphic, metamorphic, or igneous features, and fields for notes and locations, data and time stamp (color-coded)\n- the data span multiple scales from thin sections through outcrops to regions. \n- no good app exists for customized downloads and views of data, and then collecting data in the field that integrating it back into the main database. \n- lots of components exist, but have not been put together. \n- some important data in publications/papers.","- having diverse data interoperate through 3D maps layered with Digital Elevation Models. Space is the common element. ","- Data Types/sources:  legacy maps and field outcrop maps, stratigraphic data/columns, paleoenvironmental, paleontologic/fossil, geochronological, Digital Elevation Models/topography, seismic/subsurface, LiDAR and photogrammetry as point clouds, 3D remote sensing, lithological point data, geodynamic models. Historic and contemporary. We also use outcrop, hand sample, and thin section annotated photographs. So field data, model data, lab data. \n- Data Repositories: Paleo DB or ""fossil database""\n- Data Formats: mostly geospatial formats like shapefiles (ArcGIS, QGIS), KML, but a variety of data types mentioned\n- Data Volume: ","- ArcGIS, iGIS in iPad in field, 3D COLLADA models, Google Earth. GigaPans. Structure from Motion (SFM) photogrammetry software for 3D analysis. \n- For field data collection: Field Assets, StratLogger, and various photo sketching apps. I have also used GeoFieldbook, but it needs to be updated for iOS 9.\n- STRABO is a good system in development, but it is not sufficient: not customizable for field data collection. ","OGC standards for 3D modeling","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Field Mapping and Collecting Structural Field Data"
"39","Strabo Data System - Basil Tikoff","","KS","8/23/2016","Developing the Strabo Data System","Structural geology|Geographic location|Tectonophysics","Sources of variability|Long-term trends","High","Develop a data system for field geological data (thin sections, maps, and empirical data) collected on multiple scales - a lack of data availability is limiting scientific progress.","- it is hard to acquire data needed to support research. This is inefficient. There isn't a culture of data sharing.\n- data are lost or unusable (not fully documented) when a researcher retires or dies. \n- ArcGIS is good but not sufficient for storing and analyzing their spatial data. ","- not described, but it is implicit that they exist and are important. ","- Data Source/Type: Thin sections, maps, and empirical data collected from a wide selection of scales used by field geologists. A geologic map at the kilometer scale, a detailed map at the 10�s of meter scales, individual stations at the meter scale, thin sections at the millimeter scale, and then possibly even an electron miscopy sample at the sub millimeter scale. At any of those scales you could have data points, relationships, or even images linked to them. All of them are spatially referenced and are noted as such in the system\n- Data Repositories: data systems stored at Kansas. \n- Data Format: database will be Neo4j (graph database), inputs are ArcGIS or QGIS. Normal and annotated images as JPGs.\n- Data Volume: ~100MB/project, individuals probably use 10's of MB. Not highlighted as an issue. \n","- currently, ArcGIS, but it is not sufficient. \n- The Strabo system is in development and uses:Neo4j for graph DB, PHP and Python for Strabo itself. ","None listed","","Essentially this is a sales pitch for the system, Strabo, he is developing. He is not expressing is use case so much as he is saying the world will be better if it uses his tool.",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Developing the Strabo Data System"
"41","Environmental Seismology - Leslie Hsu","","KS","8/23/2016","Environmental Seismology","Geomorphology|Earth surface processes","Hazards|Predictions","High","To use broadband seismic networks and other seismic data to learn about environmental processes that create vibrations on the surface of the Earth (not earthquakes). For example, to understand sediment transport in rivers using seismic data. To better integrate seismic and non-seismic data for diverse science (geomorphology, ecology, etc.)","- Seismic data is held in IRIS, but non-seismic data are not and are difficult to discover. Making those data accessible has the usual set of challenges. \n- Need the architecture to be able to search existing infrastructure and existing databases so that the discoverability of the relevant data is improved.","1. Determine the data and data types needed to solve the question: For example: how much sediment is being transported in rivers? Seismic data can be used to address this question since the process of sediment transport creates vibrations on the ground.\n2.You need broadband seismic data, river gauge data, river discharge data, precipitation data\n3. One method is to query and see where these different data overlap: I want to know where there is a seismic station within 2km of a large river: large river defined by X discharge\n4. Search all the different data accessible and try to answer where these data overlap\n5. It�s probably in some raw or not very processed state. The next step would be to set parameters to get derived products that could be compared. \n6. Then look for correlations and analyze them to figure out some result values that can tell you about the question more broadly.\n7. Reiteration of those steps depending on subsequent results; in an ideal workflow it could be re-run multiple times.","- Data Source/Type: 1) Seismic data in IRIS and smaller investigators on short term projects that aren't in IRIS. 2) Non-seismic data: river discharge/gauge that is luckily well-organized, GIS files, imagery from satellites, photos, (landslides) informal documents and emails about events, suspended sediment and bedload sediment measurements (not spatially extensive as seismic network)\n- Data Repositories: IRIS seismology data center\n- Data Format: IRIS format, maybe SEED data, ArcGIS/QGIS files, txt, doc, photos in tif and jpg, csv. \n- Data Volume: Seismic data is GBs, non-seismic is MBs or larger\n- Data Vareity: high due to the different types/sources, and also level of processing (raw, derived, mashup)\n- Data Velocity: is continuous for seismic data. \n- Data Veracity/Quality: the database (assumed to mean IRIS) does quality contro. ","homegrown tools in MATLAB and Python. IRIS provides software. SQL for databases. ArcGIS, Excel are essential tools.","IRIS has some standards, but for the Earth surface process data, there are no larger standards (and this is a challenge).","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Environmental Seismology"
"43","Investigating volcano tectonic interactions on a global scale - D Sarah Stamps","","KS","8/23/2016","Investigating volcano tectonic interactions on a global scale. ","Tectonophysics|Volcanology|Natural hazards","Hazards","High","To better understand the role active volcanism have on global tectonic movements.","- We need millimeter precision positioning data that spans the active volcano and area around the volcano, ideally in a cross-section. \n- Being be able to compile and assess a wide range of data in one digital location is challenging. \n- Logistical challenges to collecting new data: problem with data producers (where should instruments be placed? How will they be protected?)\n- The issue of discrete data versus continuous data�some sensors record continuous data, but others are inherently discrete. For example InSAR and geochemical observations are discrete.\n- Once all that data gets compiled there would need to be some kind of statistical analysis, and that is another way EC can help. ","Desired workflow: integrated data access. (GeoMap App is an example tool that allows you to pick a region and find data that�s available. However GeoMapApp doesn�t crawl web.)\nOld workflow - without integrated discovery:To go to each database \nExtract position for each individual volcano\nPull information about geochemistry and see where the data is located\nManually process the data \nIt may require 5 years to assess data related to active volcanoes and tectonic deformation at an appropriate scale.","- Data Types: InSAR; Positioning sensors (like GPS); Petrology (geochemistry data); Temperature data; Volatiles from the off-gassing of volcanos; Gauge data; seismic data\n- Data Repositories: (further details on each given in the use case) United States Geological Survey Volcano Hazards Program.The Volcano Disaster Assistance Program (VDAP). Global Volcano Model. WOVOdat. Integrated Earth Data Applications. VHub. International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI). National Oceanographic and Atmospheric Administration (NOAA) Volcanic Ash Advisory Centers (VAACs) \n- Data Formats: GPS data in NMEA and BIMEX, Dataing data in ASCII, Geochemistry in ASCII and Excel, Temperature and GPS in ASCII, Seismic in SEED. \n- Data Velocity: some is real time (GPS and seismometers) otheres delayed mode. \n- Data Variety:  see data sources. Also field collection vs real time sensor.\n- Data Veracity/Quality: GPS data needs processing, and QC is problematic. She only trusts published data.","GAMIT-GLOBK. GMT, generic mapping tools. GeoMapApp for visualization. Google Earth�there are a number of software that allow you to output into the KMZ or KML formats, which then allow you to visualize. VIZIT visualization software. Adobe Illustrator. The IRIS web site also has some good tools to view certain mechanisms. MATLAB, QCSH, Bok. GOT for statistical analysis. LaTeX for word processing. TEQC physical analysis program.","There are some types of standards, specifically for waveform data, as well as for GPS data, but there aren�t specific standards for this specific use case.","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Investigating volcano tectonic interactions on a global scale. "
"48","Major dissolved anion and cation concentrations outside tidal zone","","KS","10/17/2016","Major dissolved anion and cation concentrations outside tidal zone (DDH)","Biogeosciences|Geochemistry|Global Change|Hydrology|Oceanography: chemical","Sources of variability|Hazards|Long-term trends","High","Understand state parameters of carbon dioxide uptake by mineral weathering reactions in drainage basins in the North America.","- data repositories can't be searched in all the ways desired\n- some data are in publications/reports/grey literature and not searchable databases\n- some data are ""dark"" (not online)\n- Not having metadata and formally defined semantics for key terms results in less efficient resource discovery.\n- datasets are discrete and sparse\n- relevant data may be taken on very different temporal scales (once per yr sample analysis vs every 15 min from in-situ sensor\n- data not in usable formats need transformations","If a user could sit on her/his computer and link to a search engine, be able to type in the name of a river in North America or select a geographic area and subsequently be able to pull up data on major dissolved anions and cations, and do so in a simple fashion, I would consider that a measure of success. Specific searches desired:\n- Critical issue is dealing with the tidal zone.  The farther the sample location is towards the ocean, but outside the tidal zone, the bigger the drainage area is that the data represent. Must know relation of data location to tidal zone.\n- Query system may correctly interpret concepts such as �major dissolved anions� or �tidal influence�, or �river�, and map them to O&M.\n- GML constructs such as �result�, �feature class�, �sampled feature�.\n- Expands the term �major dissolved anions� to a set of individual anions, concentrations of which are actually contained in the sources.\n- Then we have to define the chemical constituents themselves, major/major which have chemical names�define and utilize them effectively.\n- The same goes for units: the really necessary thing is getting access to the data.\n","- Data Sources: in-situ sensors, field data, sample data, lab analyses\n- Data Types: isotope ratios, elemental concentrations, water discharge, supporting methods information. Tidal zone location. \n- Data Formats: csv, asci, txt\n- Data repositories: Gauging stations (USGS and Environment Canada - Water Survey), Web-accessible geochemical data (USGS, Wateroffice Canada, EPA, CUAHSI-HIS), udigitized libraries/collections.\n- Data Volume: not a challenge.\n- Data Velocity: sensor data can be every 2 min. Integrating data from different velocities is a challenge, but the overall velocity is not.\n- Data Vearacity/Quality: requires sufficient metadata to assess.","- Natural language textual analysis\n- CUAHSI (Consortium of Universities for the Advancement of Hydrologic Science, Inc.)\n- LoadEst and LoadRunner (to estimate daily, monthly or annual fluxes)\n- ArcGIS or similar GIS products.\n","- Open Geospatial Consortium�s (OGC) Observation and Measurement (O&M, also ISO 19156; Cox, 2010); \n- CUAHSI�s data standards; WFS (Open Geospatial Consortium Web Feature Service Interface Standard); \n- OGC Geography Markup Language (GML, also ISO19136; Lake et al., 2004; Portele, 2007) general feature model; \n- SESAR�s IGSN (provided water samples have been registered)","","",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Major dissolved anion and cation concentrations outside tidal zone (DDH)"
"49","BioGeoSIGiPlan - Stephen Goff and Anne Thessen","","KS","9/22/2016","Predicting Best Crop Location","Global change|Public issues","Hazards|Predictions","High","Increase agricultural production by identifying which corn genes and variants are responsible for allowing corn to reach yield under different environmental conditions; allow seed producers to predict which varieties will do well in areas based on climate model predictions. This requires the analysis of genetic data with yeild information and meteorological data, soil data and other environmental data. ","- Data are distributed among multiple sources; some data is proprietary, \n49formats are heterogeneous and need manual effort to integrate\n49 analysis requires demanding computations only possible on the largest systems\n","1) Average corn yields still fall significantly short of maximum yield potential defined by genetics in ideal growing conditions. \n2) Data has to be brought together using brute force and manual methods: \n     a) All of the heterogeneous data sets from the distributed sources have to be re-entered in a new software environment and integrated around space and time. \n     b) Significant time has to be taken for downloading and reformatting�differing time scales.\n3) Weather changes all the time and corn is in the ground for several months: simple averages won�t work. \n4) Most traits of interest that have an impact on yield are complex traits, i.e. traits controlled by many different genes. So computational analysis needed to identify the genes underlying complex traits is very demanding and only feasible on the largest systems currently available unless new analysis routines become available.\n","Data formats: pdf (tables and maps), Excel, FASTA (genetic), GRIB (climate projections), netCDF, CSV, JSON, MS Access. \nData Types & Sources: corn seed genetic variation data (companies and putlic sector), climate projections (IPCC), crops location data with productivity by state and county (USDA), meteorological data (NOAA NWS), soil data (USDA Natural Resources Conservation Science). \nData Volume: ""Big""\n","Current: R, Genome Workbench, BioPython (for genetic data), netCDF library, MatLab \nProspective: GIS","NOAA, USDA, iPlant, and seed companies have standards. The hope is create a new integration and new standard.","","Nice one - potential testbed?",,,,,,,,,,,,,,,,,,,,,,,,"crimson","Predicting Best Crop Location"
"5","Protein_Portal_Full_Mak_Saito","","KS","8/1/2016","Access of Oceanic Protein Datasets","Proteomics|Ocean|Microbial ecosystems|Biogeochemistry","unclear","Low","Create a community data portal that allows research scientists to discover where, when and in which organisms a protein/enzyme of interest occurs in the oceans through a bioinformatics analysis of large mass spectral libraries created from many oceanic samples.","- mass spectrometer data distributed among multiple repositories & different mass spec platforms will pose integration problems with different datatypes.\n- How to ingest up-to-date genomic information, because protein data relies upon genome data for production.\n- volume:  .raw 10gb/day per instrument (~2-4 Tb/year/instrument), 1-2 instruments per contributing laboratory","Assuming proposed system is developed:\n  - User comes with name of a protein or sequence to the portal\n  - User enters parameters (ocean basin, year, etc)\n  - User hits Go (submits form)\n  - System matches name of the protein against a database of proteins \nSystem reports the protein found in the database along with information about the protein like where they were found, the number of spectra counts, geospatial & temporal reference frames (in table or visual [ODV form]), and the exact sequences\n  - System reports displays results in a Map and a List (spreadsheet output) with quality indicators of the data and related metadata.\n  - User evaluates results and can rerun the search with updated parameters\n  - User can download the result set (eg Ocean Data View)\nAlternate flow:\n  - User comes to portal with a geographic search (i.e., wants to see everything present)\n  - If the System found a match, it would show a table (map would be too much data) of identified proteins\n  - Otherwise, the System specifies no data found for that geographic region (region being pre-defined unit)","- Data Formats: .raw proprietary mass spectrometer-specific raw datafile, .mzXML common non-proprietary format for mass spectrometry raw data, .mzML new common non-proprietary format for mass spectrometry raw data, .mgf older format for raw data, .csv, .xml, .msf proprietary output file format, .fasta amino acid sequence database\n- Data Types: mass spectra, identified peptides after mapping to genomic datasets, spectral libraries to reduce search time, gene annotations information, relative and absolute abundance information\n- Volume: 0.6-50GB per file\n- Velocity: 2-4TB/instrument, 1-2 instruments per lab, multiple labs","ODV","none mentioned","Re: EC Science Themes: this may be baseline work needed to bring microbial community into theme research","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Access of Oceanic Protein Datasets"
"7","Permafrost_Model_Validation_Schaefer_Full","","KS","8/16/2016","Permafrost Model Validation","Permafrost|Carbon cycling","Long-term trends|Predictions|Sources of variability","Low","To gather data on permafrost characteristics from many high latitutde locations to validate and improve a carbon cycle model.  ","- researcher must access and combine data from many studies. Desired data are in Arctic Data Explorer, but are discoverable with many different search terms, making searching time consuming and laborious\n- metadata is either insufficient or time consuming to assess whether data meet the researcher's requirements\n- data are in different formats, and need to be converted (to NetCDF)\n","Use case has extensive and detailed description of desired search and access workflow. ","- Data Sources: time series datasets of permafrost temperature.\n- Data Repositories: Arctic Data Explorer (ADE) at National Snow and Ice Data Center (NSIDC)\n- Data Formats: netCDF (desired), Excel (often provided), other heterogeneous formats\n- Data Variability: format variability is a challenge\n- No issues noted for data volume, velocity, veracity. ","BCube broker; - SiBCASA (combined Simple Biosphere/Carnegie-Ames-Stanford Approach) Terrestrial Carbon Cycle Model","Excel","Using EC BCube project's broker","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Permafrost Model Validation"
"8","https://drive.google.com/drive/folders/0B-WQtNtnyJLTNHFSMjl4bExKMzQ","","LK","8/24/2016","Aeolian Processes Field Work_Martin"," ","Sources of Variability","Low","To understand the stress of wind on a barren sandy land surface in a field context (active aeolian processes). In other words, what is the amount of sand moved by the wind and turbulence? ","1) no storage location or scheme for his data, no agreement in the community\n2) data and software in 2 different places (not clear that's a big challenge for him)\n3) lack of measurement accuracy combined with need to invent conversion algorithms could lead to unreliable data","Needs better ways to share data with collaborators\n\nGithub doesn�t allow storage of large data file\nGoogle Drive stores large files but doesn�t enable SVC and development collaboration\nHard to know best way to document data for reuse by other scientists\nNo precedents or human guidance in field of Aeolian process for how to store and manage data\nNo existing systems for storing and managing Aeolian data\nDifferent terminology in Geomorphology vs. Atmospheric\nWants credit for the datasets even though he wants to share them\nTracking use of datasets (DOIs?)\n","- Data Source: �field data, including instruments, sample collections, and qualitative observations\n- Data Format: sensor outputs, such as ASCII comma delimited files; field notebook- scanned as *.pdf; spreadsheets as *.xls; photos as *.jpg\n- Volume (size): 22 GB of raw data\n- Velocity: Batch. Data are collected during 1-day or few day intervals over a period of a few weeks. Data is processed after all data have been collected. \n- Variety: Data from the sensors consist of pulse counts, voltages, serial (RS-232) outputs, and amperages. \n- Variability: temporal scope is the same for all observations (e.g., few days at a time or during one afternoon). Temporal frequency of 50 Hz (anemometers) to weather station every minute; data are collected from sand traps every hour (although interval may change depending on conditions).\n- Veracity/Data Quality (accuracy, precision): They trust the manufacturers of the sensors regarding calibration of instruments. Quality of physical measurements (e.g., sand traps) is based on standards developed by previous studies. \n- Data Types: sensors records, sand sample analyses, photos \nIdentify sensors, portals, and final and intermediate data products that your project generates as outputs.\nIntermediate data output products - *.mat files (MATLAB). ","Proprietary for data logging\nMATLAB for analyses\nExcel for spreadsheets","None","�","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Aeolian Processes Field Work_Martin"
"19","Metadata Database for Physical Samples_Hangsterfer.docx","","KS","8/19/2016","Metadata Database for Physical Samples","Oceanography|Geochemistry","physical samples supports a wide variety of science, including long-term trends","Low","To have a database for curated, oceanographic samples that was both user-friendly and curator-friendly","- she currently has to upload sample metadata into 3 different registries. They do not use the same metadata format, and reformatting is manual and time consuming.\n- four catalogs exist: all have strengths/unique roles, no one is clearly best/sufficient on its own.\n- one of the catalogs is hard to edit - have to reload all data. ","- none of the catalogs provides all the discovery features desired. Some desired elements: ""look at"" a large collection spatially, not too many hits when search on descriptors, see all samples from a cruise","- Data Repositories: NOAA IMGLS, UCSD Libraries Digital Collections; SESAR; the SIO Geological Collection web portal)\n- Data Formats: Flat files. *.xls. Associated files: images *.jpeg, analytical information about the samples. She would like to add *.avi for moving x-ray movies (e.g., CT technology � flying through the sediment core). \n- Data Volume (size): Up to 1GB per cruise with 130 cruises currently online. As more work is done on current samples and if new analytical equipment comes into the lab, this upper limit could increase.\n- Other characteristics not noted as challenges","Excel","The different repositories have different metadata templates","Contributes data to EC iSamples. ","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Metadata Database for Physical Samples"
"20","https://docs.google.com/document/d/1abfNTE0puLUMgk9gFw92uhKZ4JNXLypPwQ4VZlb37Q0/edit","","LK","8/25/2016","Reproducibly Estimating Supraglacial Lake Depth From Satellite Imagery_Pope","Cryosphere","State parameters|Long-term trends","Low","To use Landsat imagery to determine how much water is being stored in lakes on the Greenland ice sheet. ","- It is a challenge to link together project artifacts and keep it all updated and synchronized - individual datasets, code. It is not automated but manual.� new DOIs are constantly created and then manually propagated.\n- Preparing data for sharing.\n","Limited field data available, \nsharing data standards are missing (problem for others who want to use data that exists), scripts may not match data","- geotiff\n- MATLAB .mat \n- .csv\n- NASA and USGS satelite data (Landsat)\n","MATLAB (mostly)\nGDAL","None","","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Reproducibly Estimating Supraglacial Lake Depth From Satellite Imagery_Pope"
"24","https://docs.google.com/document/d/19GSyVc9RuYPzcOEMokBAhhqpyQMr9hmt0Jbj-V32doo/edit","","LK","8/26/2016","GeoLink_Semantics and Linked Data for the Geosciences_ Arko","Marine biochemistry|Ocean sciences","Preconditions?","Low","The GeoLink Building Block �Leveraging Semantics and Linked Data for Data Sharing and Discovery in the Geosciences� seeks to better connect distributed resources already available from existing infrastructure to support more efficient geoscience research. �GeoLink aims to provide improved discovery of and access to data by leveraging Semantic Web technologies and layering those additional technologies on top of existing data infrastructure. \n- understanding distribution of trace elements and isotopes in the global oceans\n\n","- Missing persistent IDs (by cruise, by dataset, and more)\n- Problems with updating�data already in \n","","","Chemical analysis\nExcel\nPhP\nHTML\nOWL","RDF ","Geolink assumes that researchers are using popular web portals. The use case describes 2 workflows - One for the end user and one for the Geolink manager","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","GeoLink_Semantics and Linked Data for the Geosciences_ Arko"
"28","https://docs.google.com/document/d/1nlLgNIZ6VpSWNC1LMLVS1MJz9GloKkc3r2TDTLThlFU/edit","","LK","8/26/2016","Top ten storms in the NARCCAP database_McGinnis","Global change","Predictions|Hazards|Sources of variability","Low","Identify intense storms in the NARCCAP dataset occurring near Fort Collins, CO\n\n","If the researcher doesn't have access to the Supercomputer at NCAR, they are unable to process the big datasets, as they don't fit on the researcher's computer.","See CI challenges - need access to and time on supercomputers, plus skills to process large data","net CDF","NCL\nWRF","CF metadata\nspecifications on NARCCARP site","","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Top ten storms in the NARCCAP database_McGinnis"
"45","https://drive.google.com/open?id=1ABkU_25GUjoRQrtNQu0-MbSnw24wYZW1Wat1h5UJDuw","","LK","9/13/2016","Finding data for sediment transport rate field studies and related resources (DDH)","Hydrology|Earth surface processes","Predictions","Low","�Better understanding of the relationship between stream flow and bedload sediment transport rates. \n\n�Flood hazard mitigiation and better understanding of longer term river evolution\n","- Ontologies are not built out effectively�including the vocabulary necessary for the queries\n- Processing queries requires concept expansion through an ontology. \n- Quite simply the sedimentary geosciences community is learning how to make these datasets available for wider use\n- Conversations between the data producers and the data publishers has started, but is not complete yet\n- In steps 4 and 5 of the basic workflow, the predicate �is close to� is also complicated in detail, because resources that are related to the same catchment area or drainage basin as the sediment transport result from step 1 would be of interest at much greater distance than studies that were not so associated. \n","There are portals for very close data but not exactly what this use case is seeking. We�re focused on bedload sediment. USGS is one producer of the data, and there are many others, but we need help aggregating these separate sources and users in order to make them effective over the longer term.\n\n","Data Sources/Repositories:\n   �USGS databases (but this is mostly for suspended sediment as opposed to bedload sediment)\n   �Disparate, small databases that are not linked because they are on personal servers or FTP sites\n\nData Formats\n   �CSV and text files are the norm\n   �For the smaller databases, something like a text format\n   �Volume (size)\n   �Non-seismic data is pretty small; on the order of megabytes handful","OGC Geography Markup Language (GML)","OGC O&M","Is a DDH use case - more information in the DDH workshop notes","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Finding data for sediment transport rate field studies and related resources (DDH)"
"14","https://docs.google.com/document/d/1zkSibtOXw2YOPdbTyPO_e-VQrzVe9tWPtA70zO32NJI/edit","","LK ","8/24/2016","Cenozoic cetacean diversity","Biogeosciences|Paleoceanography","Long-term trends","Low to medium","To compile all existing data on cetacean diversity through the Cenozoic, in order to understand what drives diversity patterns. \n","Potential solution by EarthCube: Time and location is what links everything together, and can be used to link datasets together. Every sample has coordinates and can be traced to a time period. If EarthCube could help with developing some kind of database to help resolve the chronologic challenges in his work (e.g. developing a geologic chronologic database.)\n- Sustainability\n- time investment required to enter the data, data cleaning","He used a crowd-sourced database - Paleobiology -- for paleontology to manage and organize the data. 387 people have contributed.\n\nDD challenges - Maintenance model - time investment required to enter the data, data cleaning (does GeoDeepDive offer any lessons?); completeness, sustainability, accuracy\n","*.csv\nJust working with text documents. \n\n","R\nExcel\nAdobe Illustrator","None","","This person is asking EC to help synchronize time and place between�data sources",,,,,,,,,,,,,,,,,,,,,,,,"forest_green","Cenozoic cetacean diversity"
"9","Moorea Coral Reef LTER_Edmunds.docx","","KS","8/17/2016","Moorea Coral Reef LTER","Community ecology|Marine ecology|Ecosystem science|Coral reefs","Long-term trends|Sources of variability","Med","To document changes in coral communities over different time scales, including multi-decadal, and understand the causes;  what is changing and why, and what is the vulnerability/resilience of coral reefs to different disturbances.","- remote field site with limited electricity & internet. Data must be moved off the island on physical storage media. \n- Need some kind of interface software that allows less accomplished computer/software people (like himself) sift through, conceptualize, and access the �fire hose� of data. There is a large chasm between the human capacity to think and make sense of the data, and the capability of the project to generate large quantities of data.\n- funding stability","- Too much data - hard to how to ""sift through, conceptualize, and access"" the huge amount (variety and volume)","- Data Repositories: BCO-DMO (NSF) and the Moorea data management program. Data used in manuscripts are identified with DOIs.\n- Data Sources: most data are field generated, but includes syntheses with other datasets from colleagues for context. Remote sensing, and in-situ field data. \n- Data Types: photos, counts/percent cover data from photos and transects, and measurements from biopsies of coral. On multiple spatial scales (from remotes sensing on thousands of km scale to small). \n- Data Formats: he uses .jpg, are upgrading to RAW proprietary photo format for higher resolution. *.xls, .mat. \n- Data Volume: 10s of GB per year from .jpg photos\n- Data Velocity, Veracity/Qualty: not noted as issue\n","- Analyze data using MATLAB, R, Systat, SPSS, and PRIMER.","- Data used in manuscripts get DOI.\n- They rely on broader scientific domain literature for definitions of benchmarks and best practices for determining the quality of their results and reporting","","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Moorea Coral Reef LTER"
"13","https://docs.google.com/document/d/1Z4r_hwWtPvYo68RnlP8psTuDbkUlY5HF_L9wpTrZHcQ/edit","","LK","8/23/2016","Land Use Monitoring with Unmanned Aircraft Systems (UAS)    ","Atmospheric science|Greenhouse gases","�","Med","To quantify and monitor greenhouse gas emission mitigation efforts in agricultural systems using Unmanned Aircraft Systems (UAS) (e.g., drones), with a current focus on commercial rice farming.\nTo develop UAS capabilities for collecting earth science data, both with high resolution imagery analysis and lightweight gas sensors.\n","�Challenge: Identifying sensors\nIt is difficult to find sensors that are lightweight, small, inexpensive, and collect accurate data. Efforts to overcome challenge: A) They may borrow sensors from other laboratories, B) They will test less desirable sensors (e.g., may not be as accurate) that are affordable to determine if they are appropriate for their science needs, and B) Would be helpful if X-DOMES had a search option to identify sensors according to different requirements. Currently, it is possible to only download metadata about specific sensors. �(i.e. user may want last week's earthquake data from Japan, not sensor 2345.)\n","Data discovery challenges, like CHORDS and CHILL, are about getting the right data from the requisite sensors. It is difficult to get data from the sensors� \n\nRequests: Given that they are still in the design phase and scoping (based on funding) these may well change but hypothetically and ideally:\nPortals:A CHORDS real-time portal for monitoring SSEDD enabled science UAS in an XSEDE Jetstream virtual machine available for anyone to use.A UAS data portal providing access to collected source data (dependant on finding a DAC that is interested in prototyping UAS data hosting - none currently exists)\nDataRaw image files (RAW and JPG, RBG and Infrared bands)Stitched image files (JPG, GeoTiff, *.kml, Emotion project files, .LAS (point cloud), potentially a new open source �stitched image project� file funding dependant)Raw text data logs of gas sensor voltage levels (likely in sensor manufacture proprietary formats)Text based logs of calibrated, error corrected, and unit scaled �gas sensor readings.A multi-dimensional �UAS data sensor output file� - To be determined/evaluated/developed as there currently is no standard way of encapsulating flight telemetry, with instantaneous sensor point data, with multi-channel imagery.Data plots and visualisations - web visualisations(html with D3), GeoTiff, *.kmlVideos (mp4)\n\n\nThe interviewee requested: a search option to identify sensors according to different requirements. Currently, it is possible to only download metadata about specific sensors.��\n","�Data Format: geotiff, LAS, point cloud, *.csv, geojson, \nVolume (size): <5GB\nVelocity �(e.g., real time): Data is collected from each fly over by the UAS\n","Ardupilot\nSSEDD\nOpen options, also Emotion","Ideas for future but not known yet","There's lots more info about potential data formats in the use case.� However, they are not yet sure which ones they'll use, as the project is in early stages.","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Land Use Monitoring with Unmanned Aircraft Systems (UAS)    "
"46","Finding geochemical data and sample metadata to complement investigators� data","","KS","10/17/2016","Finding geochemical data and sample metadata to complement investigators� data (DDH)","Geochemistry|Solid earth geochemistry","State parameters","Med","Compare new major, trace element, and isotopic data from the Arctic to the composition of mid-ocean ridge basalts (MORB) from around the world, by connecting data held in multiple repositories. ","In general: relevant data are held in multiple repositories, cannot be queried across in an integrated way, data are not connected across the repositories (e.g. by IGSN), and cannot be queried on all the aspects desired. ","- Overall, want to discover and access :""A collection of data from PetDB and if possible external datasets, citations for datasets, documents, or related publications, along with any metadata, images, citations, etc. that exist in external resources such as SESAR.""\n- Want to select data from a particular analytic methods (ICP-MS inductively coupled plasma mass spectrometry, and not  from LA-ICP-MS Laser Ablation Inductively Coupled Plasma Mass Spectrometry).\n- wants to find high quality data from specific rare earth elements at specific tectonic features called spreading centers.\n","- Data Sources: field data, sample data, lab analyses of samples\n- Data Types: raw, lab data, pre-digitized and recorded.\n- Data Formats: not specified.\n- Data Repositories: SESAR, PetDB, EarthChem, ""other external databases and online documents, metadata, images, and citations""\n- Data Veracity/Quality: the need to find high quality data was stated, but no indication of whether she has the info she needs to make that determination\n- Data velocity: discrete data (not mentioned as a challenge)\n- Data Volume, Variability not mentioned as challenges","None mentioned","- IGSN for samples (International Geo Sample Number)\n- repository specific data/metadata formats/content","Is a DDH use case - more information in the DDH workshop notes","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Finding geochemical data and sample metadata to complement investigators� data (DDH)"
"18","https://drive.google.com/drive/folders/0B-WQtNtnyJLTaTFXdFpHVWhRRFk","","LK","8/25/2016","Digital Rocks Portal_Prodanovic","Geology","Preconditions?","Med-high","Develop an open source and easy-to-use repository of 3D rock images (called the Digital Rocks Portal � DRP).","- Sustainability of data/images, including web pages and software to communicate with users\n-�Usability/easy browsing\n- Proper image display\n- Lack of standards\n- Many different formats used\n","\n-�Usability/easy browsing\n- Proper image display\n- Lack of standards","- Many formats used. They prefer TIFF","ImageJ\nParaview\nDjango\nDropbox/UTBox\nBitbucket\n","Author and data description standards on datacite.org\nOnes associated with DOIs","(process, not challenges)\nUsing data from the portalGo to the portal. Click on the button to browse through the available projects (e.g., images)Open the data and read the basic parameters to confirm if that data is of interest Quickly visualize the data, and download what is needed. Relevant publications can also be immediately accessed. Cross validate with the researcher�s personal work (in the future, having enough data samples in DRP is prerequisite for this step). For example, originator M.P. simulates flow through the rock and might want to cross-validate with other simulation methods or experimental data posted in DRP. Or, perhaps, she finds that the additional datasets serve to strengthen her own findings and present them in broader context.Cite and preserve the datasets and publications used in the research. \nUploading data onto the portalTo upload data, the individual opens the website and clicks on the button to: �Upload and Publish.� The individual is then taken through a variety of steps. One step: individuals are asked to provide �metadata�, which are any accompanying information that one might need to use the images. DRP are in the process of coming up with a format for this. Another step: Once the data are ready (publishable for the portal), then the dataset gets a DOI, so that they can be cited in papers. DRP pays a company to provide DOIs to all data contributors, as an incentive to participate. \n\n","",,,,,,,,,,,,,,,,,,,,,,,,"fuchsia","Digital Rocks Portal_Prodanovic"
"2","https://docs.google.com/document/d/1Mw7-K3yCerAIs6qCVoEjlbrMt_mrJDOB2GpiIrk-Xf4/edit","","LK","8/24/2016","Rapid Real-Time Atmospheric Hazards_Chandra","Atmospheric studies","Hazards|Long-term trends","Medium","To be able to observe and model atmospheric observations at real-time and multiple time scales, in order that stakeholders can respond to hazards more quickly and effectively.\n","- network speed vs. data collection velocity�- not fast enough to capture all weather phenomenon\n- Not enough sensors in critical locations\n- Younger analysts, scientists would like to see data stored in the cloud, not just public data centers\n","(These are about data collection but could also apply to saved data)\n- Geometric mapping of different data types and all of the different inputs coming at once is really hard to do\n- Figuring out time scales to take measurements at and then matching them up - for each new variable in each location, timeframe\n- Weather effects can knock out ability to collect data, causing gaps in data\n- Synchronizing data\n\n","Input: Binary formats from radarsSigmat format, the company who makes the radartimevariable, etc.\nOutput formats:NetCDF, HDF, Universal format\n\n","MATLAB\nIDL\nVCHILL\nNCAR tool","none mentioned","","EC funded",,,,,,,,,,,,,,,,,,,,,,,,"golden","Rapid Real-Time Atmospheric Hazards_Chandra"
"6","Experimental Debris flow data_Hsu.docx","","KS","8/16/2016","Experimental debris flow data","Geomorphology|Earth surface processes","Hazards|Predictions","Medium","To facilitate and make more efficient the modeling of erosion/weathering by developing community repositories for data and scripts, with examples standards for the data and examples of how to document workflows.","- There is no community repository that will take all their data or their scripts. Having better access to data and scripts would make research more efficient - she was rewriting code, as a non-programmer, that had likely already been written by someone.\n- The field has limited access to software technicians who have sufficient knowledge of their field to write, and document for reuse, needed software. Ontosoft exists, but her community has not yet bought in\n- their data volumes are large - too large for some repositories. \n- Insufficient community standards for the data she collects\n- NSF does not fund long term repositories","","- Data Source: laboratory experiments\n- Data Types: Force, video, topography (from laser and from still images. \n- Data repository: SEAD\n- Data Formats: MATLAB, .avi for videos\n- Data Volume: <1 TB for her data. But in aggregate, data volume in community is large enough to be problemantic for repository\n- Data velocity: not relevant\n- Data Veracity/Quality: instrument specific","Software used: Matlab software, Flume specific software designed by academic research group. Custom Visual Basic program to monitor sensors in real time. ","community standards do not exist and are needed","The EC Sediment Experimentalist Network (SEN) is informing what type of information should be stored (raw, processed, which metadata)? What are the best practices for documenting information, so that the information can be easily transferred to a repository? \n- EC Ontosoft could help better document and curate software and scripts, but her community must buy into it and begin using it (e.g., critical mass).\n","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Experimental debris flow data"
"10","CPO and 3D strain data integration_Mookerjee.docx","","KS","8/17/2016","CPO and 3D strain data integration","Structural geology|Tectonophysics","Sources of variability","Medium"," More efficient and complete generation of information from existing crystallographic texture data by improved community data management and sharing mechanisms, and by developing statistical models that integrate these data with three-dimensional strain data. With the ultimate goal of understanding kinematics of faults and shear zones, so that we have a greater understanding of how the structural changes inside the earth on large-time scales can be made, for instance, understanding more completely how mountains are formed.\n","- overall: good models need the integration of data from many studies and researchers. Data exist (crustallographic and 3D strain data), but are stored in personal collections. \n- Need a place to store data and analysis scripts. \n- Need to motivate community to upload/share data\n- Need middleware to translate data formats: Individual researchers are developing their own conversion scripts, which duplicates effort. Some formats are proprietary.\n- Datasets can be large - up to 1TB each - which is too large for many researchers to share on their lab websites or download for local processing\n- Need better analytic software for meta-analyses of these data - a lot of analyses are still being done by hand, inefficiently. \n- Need standards and best practices for sharing these data","- wants to online access to crystallographic and 3D strain data, as well as converstion scripts.  Much not online now. Did not specify search/discovery requirements, but could probably provide that information if asked.  ","- Data Types: crystallographic fabric/texture; 3D strains; chemistry data (ESX), EDX and EBSD data.\n- Data Formats: many. .txt. Oxford Instruments uses a *.cpr for EBSD data. Many in proprietary formats.\n- Data Repository: Strabo (in process)\n- Volume: range from <1MB to 1TB per sample. Electron Backscatter Diffraction (EBSD) produces such large datasets that they are not easily shared or even stored on an individual researchers� webpage\n- Data Variability, Veracity/Quality, Velocity not named as issuesl ","- Matlab, Mathematica scripts\n- Middleware for translating between data formats (particularly between data files generated by different analytical equipment and different equipment companies)\n- Analytical software that allows for analysis of large, integrated crystallographic and 3D datasets (existing tools for large scale analysis are not sufficient.)","None that he is aware of","EC Strabo project is critical data repository (but may need more storage)","",,,,,,,,,,,,,,,,,,,,,,,,"golden","CPO and 3D strain data integration"
"16","https://drive.google.com/drive/folders/0B-WQtNtnyJLTRkM1c1IzNzc2dm8","","LK","8/25/2016","Deformation of active volcanoes_Freymueller","Geodesy and gravity","None volunteered. Could be: Hazards, Long -term trends","Medium","To understand how, where, and when magma accumulates underneath active volcanoes, and what is involved in the eruption process for getting magma up the surface.","- latency in data collection of weeks\n- Quality control of data\n- Having accurate and up-to-date metadata from multiple contributors\n- Usability of InSAR data","See CI Challenges","Formats: RINIX � for raw data; time series of positions for each site are stored in a simple ASCII file with fields separated by whitespace\n\nData types: GPS and InSAR\n","MATLAB\nJPL - GIPSY/OASIS\nC, Fortran (inhouse)","Processed by UNAVCO to turn into metadata","Also interested in improved visualization and modeling, including via standardized tools. Scriptable interactive tools for visualization in particular","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Deformation of active volcanoes_Freymueller"
"21","Intermittent Aeolian Transport on Earth and Mars_Swann.docx","","KS","8/19/2016","Intermittent Aeolian Transport on Earth and Mars","Planetary Sciences|Solid Surface Planets","Prediction|Sources of Variability","Medium","To use lab exeriments to observe and quantify the threshold  and characterize the modes of movement of aeolian sand transport on Earth and Mars; to improve models of aeolian transport.","- field data from two instruments must be manually integrated. Having one acquisition system would help. \n- she collects data on her laptop, and it fills up. Would be better if data were collected on a computer with more space. It also takes time and money to make copies of the data for backup. \n- Primary challenge: there is no repository for video and flow velocity data. She gets comparison data out of publications, which limits the field's progress. \n- insufficient experimental protocols and standards exist.","There is no repository where she can find comparative data - she is searching for data in literature. ","- Data Sources: Wind tunnel experiments. \n- Data Formats: *.dat that gets changed to *.txt for Matlab, and then becomes *.mat. Images are *.avi. Movies are *.mp4 and *.mov.\n- Data Volume: She saves most data at *.dat (reduced size), so she has lots of data in files of about 40-45 MB. Data volume is a challenge. \n- Data Velocity: They generate about 7GB + 40-80 MB, per day.\n- Data Variety: different instruments provide different formats. ","Software: VLC to splice videos, Matlab for analysis. They take *.mpeg from VLC and bring it into Matlab to extract the frames. ","None mentioned. Lack of experimental protocols is a challenge","","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Intermittent Aeolian Transport on Earth and Mars"
"32","https://drive.google.com/open?id=11Oq0O_v3S4xn4wtxXQ7XYYVgZ5iB1hzHSFj-c8NqdhI","","LK","9/13/2016","Reconstructing ancient rivers sedimentology from the  Paleocene-Eocene thermal maximum in Big Horn Basin Wyoming.","Hydrogeology|Global change","Long-term trends|Sources of variability|State parameters","Medium","The goals here are to use the stratigraphic record to infer past changes on the Earth�s surface related to continental rifting.","- Data are complex (field data, photos, samples, etc.), heriarchical, and have relationships between them - no system does this. Managing time is also challenging. Do not have any standards for how to handle. \n- Funding lag for long-term data storage. NSF - unsure about, NIH has promised to fund some. Want it to sustainable, beyond on NSF funding cycle.","- Data is stored both in SEAD, an NSF -funded project online database and also in SEN (an RCN).   Also GeoPRISMs. But the PIs data is stored in lots of diverse locations. This seems to be on purpose because Kyle Straub is a test case (for what?)\n- No obvious way to combine experiments to numerical models (models are often in CSDMS)\n- Funding lag for long-term data storage","DEMs of topography in ASCII, xy attributes. \nBinary data stored by instrument, often proprietary format. ","R\nMATLAB\nExcel\nDelft3d","Some SEASAR, IDEA\nComplicated story","They use MATLAB for data visualization and analytics, PETREL, and other software, including Excel for visualization, too (Why 2 tools for visualization?)","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Reconstructing ancient rivers sedimentology from the  Paleocene-Eocene thermal maximum in Big Horn Basin Wyoming."
"33","Storage and use of experimental sedimentology data - Kyle Straub","","KS","8/22/2016","Storage and use of experimental sedimentology data","Earth surface processes|Stratigraphy|Hydrology|Marine geology","Long term trends|Prediction","Medium","Understanding how stratigraphic surfaces are related to changes in geomorphic surfaces that existed at certain times on the Earth�s surface through both experiments and modeling. ","- My data is currently stored in diverse locations for different people in my community and in different databases. They have been mostly good about allowing me to utilize different places because I�m considered a test case.\n- One of the problems linking experiments to numerical models is that often the numbers that exist between field and numerical models versus those collected via experimentation are challenging to deal and there is not standard or uniform means of combining this data.\n- There is a challenge to see if our models are producing similar stratigraphic models after multiple runs but we don�t have the temporal depth to be able to measure all these model runs over a long period of time.\n- Funding and reliability of funding for CI is uncertain. \n- PIs need to understand what they should be doing with data, and this ""needs to come from NSF, a roadmap, because we have enough challenges as it is. ""\n","- the end goal is to connect expiremental data and models, but discovery vision not given. ","Data Repositories: SEAD (Sustainable Environment through Actionable Data), GeoPRISMS (Geodynamic Processes at Rifting and Subducting Margins), SEN (Sediment Experimentalists Network), his personal holdings (and presumably those of similar researchers)\nData Types: Numerical models; LIDAR and topographic maps; seismic data; experiments in lab and field sediment data.\nData Formats: DEMs of topography ar ASCII xy, SG for subsurface stratigraphy and others, binary data formats - often proprietary, header information about grain size distribution\nData Volume: Any one map could be 100mbs, and we�re collecting 100 of them, so then you have large masses that include sizes less than 1mb up to 100mbs\nData Veracity/quality: Some of the issues that we have with respect to data quality are topographic measurements that are well below 1mm in accuracy. We have challenges with the LIDAR data and how quickly it can be collected at higher spatial resolutions, but you want them to run in a reasonable format and how often you are going to run them is important. There are similar problems when measuring using instrumentation, including Doppler measurements: how accurately you can relate these is a chronic challenge.\n","Software: Primarily our work falls between MATLAB at the field-scale and seismic interpretation packages developed by the energy industry. These include the industry standard Kingdom Suite. Another program used is called PETREL. Excel for visualization, of course. R and Python, other than MATLAB and PETREL, are the two other significant programs we use in our lab.\n- CSDMS is a source of models. ","We have our own best model for our data, which we have carried over to SEAD.","Uses EC SEN project, which stores information about experiments that have been done in the past and provides link for where it�s to be stored. ","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Storage and use of experimental sedimentology data"
"37","Chemical properties of surface and groundwater - Claire Lunch","","KS","8/23/2016","Chemical properties of surface and groundwater","Biogeosciences|Hydrology|Global change|Informatics","State parameters|Long term trends","Medium","Facilitate diverse ecological forecasting through a better data management system for water/groundwater chemical analyses that can streamline data uploads, relate different data taken at the same site/time, and better user access. ","- diverse data, many data providers, diverse users. Relationships between related data not currently captured.\n- Data are not currently all centralized.\n- need better QC associated with ingest.","Most of the instrumented data, like towers measuring atmospheric variables and the like are data set in time series with a time stamp and a value, time stamp and a value, etc. You can have millions of these time series for each instrument. From a database point of view this is relatively simple. When you start looking at the observational data, however, it is very different: the data isn�t simply a time stamp and a value anymore. You might have context, such as �I measured water chemistry on this water sample X,� and �I also measured these other products on this other sample.� So it gets more complicated. We need that linkage built into to system, such that a sample taken will ultimately build toward a database that has all the interrelations and is more complicated than just one simple product. ","- Data Source/Type: analyses of water samples, with sample lat/lon, date, and elevation, etc.\n- Data Repositories: NEON data portal\n- Data Format: mainly CSV on collection side. Oracle back end. EML for metadata\n- Data Volume: not an issue. Largest is multiple MG per year for all the sites.\n- Data Veracity/Quality: is a concern. Writing down sample data incorrectly is an issue. Can be date wrong, sorted wrong, ented twice, etc. \n","Oracle database back end. Liferay software. ","Ecological Metadata Language (EML) for metadata ","","This use case lacked some clarity in that the overall NEON goals and a more specific water chemistry project were conflated. And NEON seems to be building the system they want, so it isn't clear if there will be an unmet need.",,,,,,,,,,,,,,,,,,,,,,,,"golden","Chemical properties of surface and groundwater"
"38","https://drive.google.com/open?id=11J2vFCnAnVpmugCw778F5MZJZISgUDhItWHD6KG4esA","","LK","9/13/2016","GEO-inspired Data-Enabled Materials in Aid of Society (GEO-DE MAS)","Mineral physics","State parameters|Hazards|Predictions","Medium","to understand chemical and physical properties and behavior of minerals for better disaster prediction (volcano eruptions, earthquakes). On the technological side, to develop better materials for engineering, electronics, energy storage, etc. by learning from minerals and other geo-materials. \n\n","-Need more funding and more support for putting the entries into the database to keep up with competition. Currently 50,000 entries.\n- Want databases that are accessible. The databases should be more comprehensive and cover the right disciplines (seismology, for example)\n- Need training on experiments, how to run simulations\n- Need more efforts, people to build good, stable software for the community","-Databases aren't comprehensive, accessible","�Data Sources: \n  - various forms of digital imagery, including x-ray diffraction and fluorescence, as well as measurements of thermodynamic properties that include e.g. melting temperatures, thermal expansion, etc. The synchrotron facilities provide much of this data.\n  - Density functional theory calculations\n�Data Formats: \n  - The x-ray images are stored in standard graphics format (e.g. tiff), as in proprietary compressed image format specific to detector manufacturer. \n  - Crystallographic information is stored in standardized Crystallographic Information File (CIF) format, established by the International Union of Crystallography (IUCr). There is a commission that oversees the definition of these standards and keeps them up to date. \n  - Some of the data (e.g. Raman spectra, integrated powder diffraction patterns) are stored as simple ASCII files.","Excel\nIDL\nMATLAB\nOpen-source, free s/w developed at universities for analysis of crystallographic experimental data","Unclear","More on Data Analytics: We look at serial data (as a function of pressure, temperature, composition or time), and try to determine continuous trends, as well as identify discontinuities. Some data visualization capabilities are provided by the specialized data analysis software, but much of it is done with standard tools such as Excel, IDL, or Matlab. Our online databases currently do not provide visualization solutions.\n\nMore on software for analyzing experimental data is contained in the use case\n","",,,,,,,,,,,,,,,,,,,,,,,,"golden","GEO-inspired Data-Enabled Materials in Aid of Society (GEO-DE MAS)"
"44","https://drive.google.com/open?id=1qjBESWE6KjinbODPn2j3_URvVCmnl8qnNI-_m9lQvSI","","LK","9/13/2016","Convert raw sequence data into biologically relevant information","Biogeosciences|Informatics|Oceanography: biological and chemical","Sources of variability|State parameters|Long-term trends","Medium","After data retrieval, to effectively translate raw sequence data into a format that provides genomic and phylogenetic context. In order to understand complex Earth Systems, most sequence-related data sets require a workflow to process raw data into a format that has meaning in the context of biological interactions (e.g. How microbes affect Earth systems, each other, and vise versa). The general idea is to find �who is where and what they are doing� related to sequence data.","- Need documentation standards\n- large data sets require nonlinear increases in computation\n- The community needs a common resource that can be used for most data scales and projects that can be used to standardize work between small and large labs.\n","�Database limitations � microbial community members may not be represented (may not exist)\n�Large portions of seq data sets are unknown/unidentifiable (have never been identified and are discovery-based)\n�On-going community data submissions are slowly expanding reference databases; it takes while to get things QC�d and is out of our hands\n�Major databases are working to clean up their data repositories�they are always improving","�.fasta common non-proprietary format for basic sequence data\n�.fastq common non-proprietary format for sequence data linked with quality data\n�.tsv\n�.csv\n\n�Outputs may vary depending on the tool  and may have different file extensions that limit compatibility","","Stds determined by Genomic Standards Consortium: MIMS/MIGS and MIxS","","",,,,,,,,,,,,,,,,,,,,,,,,"golden","Convert raw sequence data into biologically relevant information"
"36","https://drive.google.com/open?id=0BzqmuwuUSSbtcjN6Qk1iaENEWjQ","","LK","9/13/2016","4D visualization","3D visualization for geology","Long-term trends|Hazards","Medium to high","To develop and build four-dimensional geologic models that are of use to researchers in diverse\n\ncommunities within the geosciences.","- The big challenge is the software necessary to undertake 4D visualizations to this point. There aren�t really any killer applications currently, and all of it has become quite cumbersome--we�re kludging together data sources. ArcGIS is designed for application specialists, not for the needs of geologists\n- Need an environment for data management - An industry/academic partnership to do that?  A facility that stores sophisticated maps? A library","Data is on people's computers, not accessible\nAnd when you do assemble, resulting data is large\nAlso the detail level of the data can be very variable.\nUSGS isn't addressing the needs","\n�Data Source\n? Geochronology\n�Field data\n�Stratigraphic data, including fossil\n\�Obscure things that are hard to quantify: x is older than y, in a way that is built into geologic thinking�comparative sources\n\n�Data Format\n? Map info that is input into GIS and then adapted into a 3D GIS database \n�Simple paper map scanned as a raster.\n�Geochronology sources\n�Stratigraphic formats from available software \n�Satellite imagery, including LIDAR formats","Move\nEarthVision, etc. \nParaview\n","None","Formats for the 4D visualization use several different formats. Midland Valley makes a software called Move you can�t get out into other formats. Google Collada can be adapted for Google Earth for 3D visualization, but not 4D.\n","",,,,,,,,,,,,,,,,,,,,,,,,"fuchsia","4D visualization"
"12","https://drive.google.com/drive/folders/0B-WQtNtnyJLTTl9LNmFsUkNjdU0","","LK","8/24/2016","Magnetosphere-Ionosphere-Atmosphere Coupling (MIAC) project_Gjerloev","Atmospheric|Geospace","Long-term trends|Predictions|Sources of variability","Medium?","To provide complete global and continuous electrodynamic solution of the atmosphere. Solar wind (plasma flowing away from sun), magnetosphere, and upper atmosphere (e.g., ionosphere) interact in complex fashion to affect the Earth�s space (e.g., space weather). To understand how these interactions work, it is important to be able to determine the complete electromagnetic solution of the auroral ionosphere. ","Didn't have or have access to the required data sets previously. Now, with the existence of 3 data systems, they do.\nRemaining issue related to CI is maintenance of these systems.\n","Big Data - 1 TB per year, 20 GB of data per data. ","They are combining data from the three different systems, so the data are all different. SuperDARN: collected data generated from 17 radars around the world; AMPERE: data are generated mostly from satellites; SuperMAG: data are sourced from more than 300 magnetometers located around the world (e.g., about 100 nations). All are vector measurements that represent time series data collected at different locations. ","Developed at APL","For the data format - CDF, but there are none for the community","�","This is a source of data that should be accessible via the DDH portal",,,,,,,,,,,,,,,,,,,,,,,,"golden","Magnetosphere-Ionosphere-Atmosphere Coupling (MIAC) project_Gjerloev"
"23","Storage and Use of High-resolution Commercial Imagery by NSF Investigators","","KS","8/20/2016","Storage and Use of High-resolution Commercial Imagery by NSF Investigators","Topography|Geomorphology","Good topographic maps are a foundation for a wide variety of science (inform climate change, sea level rise, tectonics, civil engine, structural geology, ecology, etc)","The work itself is Low, but supports diverse science","To complete high resolution (submeter) elevation maps of the polar regions that are useful for the science community","- large volumes of data must be stored, and hard to find funding for it. He would like to see a community cloud storage facility. \n- data for some regions must be requested and can take a day to several months to be delivered, and it not available to all (only NSF-funded projects and govt)","Discovery not mentioned as a challenge per se. More about access after discovery. ","- Data Sources: satellite images, Digital Elevation Model output\n- Data Types: satellite imagery, digital elevation models output, imagery mosaics. \n- Data Repositories: National Geospatial Intelligence Agency (NGA) \n- Data Formats: *.ntf (image data used by defense and intelligence for remote sensing). GeoTIF for elevation data and some imagery products.\n- Data Volume: Petabyte a year. They hold about 3-5 petabytes. \n- Data Velocity: 4 satellites are collecting imagery, 24 hours a day\n- Data Veracity/Qualitu: is managed by NGA repository.","Standard geospatial software GIS and remote sensing).  ARCGIS, Imagine, ENVI, GDAL and many open source packages.","","Blue Waters HPC is a critical resource","",,,,,,,,,,,,,,,,,,,,,,,,"aquamarine","Storage and Use of High-resolution Commercial Imagery by NSF Investigators"
"40","https://drive.google.com/open?id=1hkuxHiQ4qKXiojqYfXCIBlyNHPscXfUsMRRP3XSz9KY","","LK","9/13/2016","Isotopic Composition of Precipitation","","","","To undertake paleoclimate studies over time, such that you one is able to acquire information about precipitation rate, evaporation flux, etc.. Long terms trends are the overarching drivers for the paleoclimate.","- Navigating the data sites can be a nightmare - Sites should be search friendly and navigable","- Problems using IAEA data\n- A plug in for Google earth doesn't work outside North America\n- Get data from other random sources\n- Wants a public site that has everything\n- The biggest challenge may be whether or not the data exists in certain locations to begin with. There may not be data sufficient for precipitation on top of Mt. Everest as one extreme. It�s possible this could all be complicated by the fact that some of what we�re asking for hasn�t been recorded yet.\n- Resolution of the data - it varies","- Mostly from Excel","Excel\nISODAT\nPowerPoint\nGoogle earth\nOrigin (for similar work)","None","","",,,,,,,,,,,,,,,,,,,,,,,,"beige","Isotopic Composition of Precipitation"
"42","Data Management for Ocean Carbon Dioxide Measurements - Repeated Section Hydrography - Alex Kozyr","","KS","8/24/2016","Data Management for Ocean Carbon Dioxide Measurements: Repeated Section Hydrography","Oceanography","State parameters|Long-term trends","","To manage the data that is developed from CO2 measurements in seawater: total carbon dioxide, dissolved inorganic carbon, salinity, partial pressure, and pH. In order to: \n-  Calculate anthropogenic CO2 in the ocean; scientists use this to separate manmade changes throughout a given year\n-  Determine how much is sunk into the ocean. Calculations made give us sense of ocean absorption.\n-  Repeat sections determine how much these measurements change, for example pH level\n","-  Uniting biological and hydrographic data. This needs integration/interoperability among data centers, and also community guidance. \n-  Data quality is a challenge: the quality of measurements can vary among groups collecting them. Some data needs to be thrown out. He frames this as a funding problem, not a CI problem.\n-  Metadata is/was not sufficient to know critical factors about how a water sample was taken and analyzed. ","Not detailed but could perhaps be extracted for the biological/hydrographic integration he suggests.","- Data Sources: chemical measurements from in-situ sensors and analyzed water samples collected from ships and moorings/buoys.\n- Data Repositories: Mercury, Web-Accessible-Visualization System (WAVS), CDIAC\n- Data Formats: CSV or TSV spreasheets coming in, CSV and NetCDF for publishing out. Images, simulated sky maps. XML. \n- Data volume: 100's of MB to GBs\n- Data Velocity: not clear if he is working with real-time data, or delayed mode. ","WAVS data synthesizes data automatically and produces visualization relationships between databases (developed within project)\nOcean Data View - for data manipulation and QC\nExcel\nCDIAC program for CO2 calculation app","DOIs for data. Mercury metadata.","","Alex is the system developer - he is not an end user scientist.",,,,,,,,,,,,,,,,,,,,,,,,"beige","Data Management for Ocean Carbon Dioxide Measurements: Repeated Section Hydrography"
"47","CHORDS (Cloud-Hosted Real-time Data Services for the Geosciences) � Access of Real Time Data","","KS","10/18/2016","Cloud-Hosted Real-time Data Services for the Geosciences","Hydrology|Natural hazards","Hazards|Predictions","","Create a cloud-based infrastructure for real-time geosciences data needed for improved flood forcasts and other applications. Provide the best possible estimate of what is happening on the ground at any time and then use it to forecast into future.","- real-time data acquisition & management\n- Scaling the architecture up to the number of sensors","","- Data Types & Sources: real time field sensor point data (weather sensors). Radar (model output as product/future goal.) \n- Data repositories: ""a number of repositories"" including for radar data\n- Data Formats: csv, NetCDF, SensorML, ""rudimentary"" database\n- Data Volumes: could be huge. High resolution radar, many streams\n- Data Velocity: real time or near real time\n- Data Veracity/Quality: need QA/QC on real-time data\n","CHORDS visualization\nSEEK\nJavaScript\nOnline script with geospatial software that supports raster and general web site frameworks","SensorML\nNetCDF","1. Start with a data logger connected to the Internet; \n2. Go on Amazon or other cloud provider and download it once your CHORD instance has been launched; \n3. Create a sensor feed with however many sensors so that each is streaming the data; \n4. You put together the sensor notes and post the HTTP address after copying and pasting; \n5. Stream data into CHORDS; \n6. Develop real-time models simultaneously (sensorML models already connected)","",,,,,,,,,,,,,,,,,,,,,,,,"beige","Cloud-Hosted Real-time Data Services for the Geosciences"